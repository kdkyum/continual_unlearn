{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07463560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76d3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found methods: ['FT_continual_unlearn', 'retrain_continual_unlearn', 'NG_continual_unlearn', 'synaptag_continual_unlearn', 'boundary_shrink_continual_unlearn', 'GA_continual_unlearn', 'RL_continual_unlearn', 'boundary_expanding_continual_unlearn']\n",
      "Method FT_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method retrain_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method NG_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method synaptag_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method boundary_shrink_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method GA_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method RL_continual_unlearn has datasets: ['cifar10', 'cifar100']\n",
      "Method boundary_expanding_continual_unlearn has datasets: ['cifar10', 'cifar100']\n"
     ]
    }
   ],
   "source": [
    "# Base directory for checkpoints\n",
    "base_dir = \"/u/kdkyum/ptmp_link/workdir/continual_unlearn/checkpoints\"\n",
    "\n",
    "# Function to find all available methods with continual_unlearn suffix\n",
    "def find_methods():\n",
    "    methods = []\n",
    "    if os.path.exists(base_dir):\n",
    "        for item in os.listdir(base_dir):\n",
    "            if item.endswith(\"_continual_unlearn\") and os.path.isdir(os.path.join(base_dir, item)):\n",
    "                methods.append(item)\n",
    "    return methods\n",
    "\n",
    "# Get all methods\n",
    "methods = find_methods()\n",
    "print(f\"Found methods: {methods}\")\n",
    "\n",
    "# Identify available datasets for each method\n",
    "datasets = {}\n",
    "for method in methods:\n",
    "    method_dir = os.path.join(base_dir, method)\n",
    "    datasets[method] = [d for d in os.listdir(method_dir) if os.path.isdir(os.path.join(method_dir, d))]\n",
    "    print(f\"Method {method} has datasets: {datasets[method]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980499d",
   "metadata": {},
   "source": [
    "Now let's create functions to load and process the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac58864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_results(method, dataset):\n",
    "    \"\"\"Load evaluation results for a specific method and dataset\"\"\"\n",
    "    results = []\n",
    "    method_dir = os.path.join(base_dir, method, dataset)\n",
    "    \n",
    "    if not os.path.exists(method_dir):\n",
    "        print(f\"Directory not found: {method_dir}\")\n",
    "        return results\n",
    "    \n",
    "    # Get all forget stages\n",
    "    forget_stages = []\n",
    "    for stage_dir in os.listdir(method_dir):\n",
    "        stage_path = os.path.join(method_dir, stage_dir)\n",
    "        if os.path.isdir(stage_path):\n",
    "            try:\n",
    "                # Handle both underscore and hyphen formats (e.g., '0_1' or '0-1')\n",
    "                if '-' in stage_dir:\n",
    "                    begin, end = map(int, stage_dir.split('-'))\n",
    "                elif '_' in stage_dir:\n",
    "                    begin, end = map(int, stage_dir.split('_'))\n",
    "                else:\n",
    "                    # Skip directories that don't follow either pattern\n",
    "                    raise ValueError(f\"Directory name format not recognized: {stage_dir}\")\n",
    "                    \n",
    "                forget_stages.append((begin, end, stage_dir, stage_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping directory with invalid format: {stage_dir} - {str(e)}\")\n",
    "    \n",
    "    # Sort by end class for proper ordering\n",
    "    forget_stages.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Load results for each stage\n",
    "    for begin, end, stage_dir, stage_path in forget_stages:\n",
    "        eval_file = os.path.join(stage_path, 'evaluation_results.json')\n",
    "        if os.path.exists(eval_file):\n",
    "            try:\n",
    "                with open(eval_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    results.append({\n",
    "                        'method': method,\n",
    "                        'dataset': dataset,\n",
    "                        'forget_class_begin': begin,\n",
    "                        'forget_class_end': end,\n",
    "                        'data': data,\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {eval_file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_metrics(results):\n",
    "    \"\"\"Extract key metrics from loaded results into a structured DataFrame\"\"\"\n",
    "    metrics_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        method = result['method']\n",
    "        method_display = method.replace('_continual_unlearn', '')\n",
    "        dataset = result['dataset']\n",
    "        forget_begin = result['forget_class_begin']\n",
    "        forget_end = result['forget_class_end']\n",
    "        data = result['data']\n",
    "        \n",
    "        # Extract common metrics\n",
    "        metrics = {\n",
    "            'method': method_display,\n",
    "            'dataset': dataset,\n",
    "            'forget_class_begin': forget_begin,\n",
    "            'forget_class_end': forget_end,\n",
    "            'classes_forgotten': forget_end - forget_begin,\n",
    "            'unlearning_time': data.get('unlearning_time', None)\n",
    "        }\n",
    "        \n",
    "        # Extract accuracy metrics\n",
    "        if 'accuracy' in data:\n",
    "            if isinstance(data['accuracy'], dict):\n",
    "                for key, value in data['accuracy'].items():\n",
    "                    metrics[f'accuracy_{key}'] = value\n",
    "            else:\n",
    "                metrics['accuracy'] = data['accuracy']\n",
    "\n",
    "        for x in data[\"class_wise_accuracy\"]:\n",
    "            metrics[f'accuracy_class_{x[\"class\"]}'] = x.get('accuracy', None)\n",
    "        \n",
    "        # Extract MIA metrics\n",
    "        if 'SVC_MIA_forget_efficacy' in data:\n",
    "            for key, value in data['SVC_MIA_forget_efficacy'].items():\n",
    "                metrics[f'mia_forget_{key}'] = value\n",
    "                \n",
    "        metrics_data.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a4b90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping directory with invalid format: masks - Directory name format not recognized: masks\n",
      "[{'class': 0, 'total_samples': 4500, 'correct_counts': 84, 'accuracy': 1.8666666746139526, 'dataset': 'train'}, {'class': 1, 'total_samples': 4500, 'correct_counts': 4490, 'accuracy': 99.77777862548828, 'dataset': 'train'}, {'class': 2, 'total_samples': 4500, 'correct_counts': 4456, 'accuracy': 99.02222442626953, 'dataset': 'train'}, {'class': 3, 'total_samples': 4500, 'correct_counts': 4253, 'accuracy': 94.5111083984375, 'dataset': 'train'}, {'class': 4, 'total_samples': 4500, 'correct_counts': 4434, 'accuracy': 98.53333282470703, 'dataset': 'train'}, {'class': 5, 'total_samples': 4500, 'correct_counts': 4200, 'accuracy': 93.33333587646484, 'dataset': 'train'}, {'class': 6, 'total_samples': 4500, 'correct_counts': 4445, 'accuracy': 98.77777862548828, 'dataset': 'train'}, {'class': 7, 'total_samples': 4500, 'correct_counts': 4453, 'accuracy': 98.95555877685547, 'dataset': 'train'}, {'class': 8, 'total_samples': 4500, 'correct_counts': 4486, 'accuracy': 99.68888854980469, 'dataset': 'train'}, {'class': 9, 'total_samples': 4500, 'correct_counts': 4446, 'accuracy': 98.80000305175781, 'dataset': 'train'}, {'class': 0, 'total_samples': 1000, 'correct_counts': 26, 'accuracy': 2.5999999046325684, 'dataset': 'test'}, {'class': 1, 'total_samples': 1000, 'correct_counts': 979, 'accuracy': 97.9000015258789, 'dataset': 'test'}, {'class': 2, 'total_samples': 1000, 'correct_counts': 945, 'accuracy': 94.5, 'dataset': 'test'}, {'class': 3, 'total_samples': 1000, 'correct_counts': 803, 'accuracy': 80.30000305175781, 'dataset': 'test'}, {'class': 4, 'total_samples': 1000, 'correct_counts': 936, 'accuracy': 93.5999984741211, 'dataset': 'test'}, {'class': 5, 'total_samples': 1000, 'correct_counts': 835, 'accuracy': 83.5, 'dataset': 'test'}, {'class': 6, 'total_samples': 1000, 'correct_counts': 945, 'accuracy': 94.5, 'dataset': 'test'}, {'class': 7, 'total_samples': 1000, 'correct_counts': 947, 'accuracy': 94.69999694824219, 'dataset': 'test'}, {'class': 8, 'total_samples': 1000, 'correct_counts': 979, 'accuracy': 97.9000015258789, 'dataset': 'test'}, {'class': 9, 'total_samples': 1000, 'correct_counts': 945, 'accuracy': 94.5, 'dataset': 'test'}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(method_results)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame for easier analysis\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Show basic stats\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evaluation results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 83\u001b[0m, in \u001b[0;36mextract_metrics\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_wise_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_wise_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_wise_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[1;32m     85\u001b[0m         metrics[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# Load all results\n",
    "all_results = []\n",
    "for method in methods:\n",
    "    # for dataset in datasets[method]:\n",
    "    method_results = load_evaluation_results(method, \"cifar10\")\n",
    "    all_results.extend(method_results)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = extract_metrics(all_results)\n",
    "\n",
    "# Show basic stats\n",
    "print(f\"Loaded {len(df)} evaluation results\")\n",
    "print(f\"Methods: {df['method'].unique()}\")\n",
    "print(f\"Datasets: {df['dataset'].unique()}\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f741aa",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Now, let's create visualizations to show the performance of different methods across sequential unlearning stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18bb1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the ground truth method\n",
    "ground_truth_method = 'retrain'\n",
    "\n",
    "def plot_metric_by_dataset(dataset, metric, title=None, ylim=None):\n",
    "    \"\"\"Plot a specific metric for all methods in a dataset\"\"\"\n",
    "    # Filter data for the specific dataset\n",
    "    dataset_df = df[df['dataset'] == dataset]\n",
    "    \n",
    "    if len(dataset_df) == 0:\n",
    "        print(f\"No data available for dataset {dataset}\")\n",
    "        return\n",
    "        \n",
    "    if metric not in dataset_df.columns:\n",
    "        print(f\"Metric '{metric}' not found in data. Available metrics: {[col for col in dataset_df.columns if col not in ['method', 'dataset', 'forget_class_begin', 'forget_class_end', 'classes_forgotten']]}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each method\n",
    "    for method in dataset_df['method'].unique():\n",
    "        method_data = dataset_df[dataset_df['method'] == method]\n",
    "        \n",
    "        # Sort by forget_class_end to ensure proper sequence\n",
    "        method_data = method_data.sort_values('forget_class_end')\n",
    "        \n",
    "        if method == ground_truth_method:\n",
    "            # Highlight ground truth method\n",
    "            plt.plot(method_data['forget_class_end'], method_data[metric], \n",
    "                    marker='o', linewidth=3, markersize=10, label=method, linestyle='-', color='black')\n",
    "        else:\n",
    "            plt.plot(method_data['forget_class_end'], method_data[metric], \n",
    "                    marker='o', linewidth=2, markersize=8, label=method)\n",
    "    \n",
    "    if title is None:\n",
    "        title = f\"{metric} vs. Classes Forgotten ({dataset})\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Cumulative Classes Forgotten')\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_gap_to_ground_truth(dataset, metric, title=None):\n",
    "    \"\"\"Plot the gap between each method and the ground truth method\"\"\"\n",
    "    # Filter data for the specific dataset\n",
    "    dataset_df = df[df['dataset'] == dataset]\n",
    "    \n",
    "    if len(dataset_df) == 0 or ground_truth_method not in dataset_df['method'].unique():\n",
    "        print(f\"Dataset {dataset} doesn't have ground truth method {ground_truth_method}\")\n",
    "        return\n",
    "        \n",
    "    if metric not in dataset_df.columns:\n",
    "        print(f\"Metric '{metric}' not found in data\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get ground truth data\n",
    "    ground_truth_data = dataset_df[dataset_df['method'] == ground_truth_method]\n",
    "    ground_truth_dict = dict(zip(ground_truth_data['forget_class_end'], ground_truth_data[metric]))\n",
    "    \n",
    "    # Plot gap for each method\n",
    "    for method in dataset_df['method'].unique():\n",
    "        if method == ground_truth_method:\n",
    "            continue\n",
    "            \n",
    "        method_data = dataset_df[dataset_df['method'] == method]\n",
    "        method_data = method_data.sort_values('forget_class_end')\n",
    "        \n",
    "        # Calculate gap at each stage\n",
    "        gaps = []\n",
    "        x_values = []\n",
    "        \n",
    "        for _, row in method_data.iterrows():\n",
    "            stage = row['forget_class_end']\n",
    "            if stage in ground_truth_dict:\n",
    "                gap = abs(row[metric] - ground_truth_dict[stage])\n",
    "                gaps.append(gap)\n",
    "                x_values.append(stage)\n",
    "        \n",
    "        if gaps:\n",
    "            plt.plot(x_values, gaps, marker='o', linewidth=2, markersize=8, label=method)\n",
    "    \n",
    "    if title is None:\n",
    "        title = f\"Gap to Ground Truth ({ground_truth_method}) for {metric} ({dataset})\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Cumulative Classes Forgotten')\n",
    "    plt.ylabel(f\"Absolute Difference from {ground_truth_method}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1b08d",
   "metadata": {},
   "source": [
    "## Class-wise Accuracy Analysis\n",
    "\n",
    "Now let's analyze the per-class accuracy for different methods at various stages of continual unlearning. This will show how unlearning affects the model's performance on individual classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236e9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classwise_accuracy(method, dataset):\n",
    "    \"\"\"Extract class-wise accuracy from evaluation results for a method and dataset\"\"\"\n",
    "    classwise_data = {}\n",
    "    method_dir = os.path.join(base_dir, method, dataset)\n",
    "    \n",
    "    if not os.path.exists(method_dir):\n",
    "        print(f\"Directory not found: {method_dir}\")\n",
    "        return classwise_data\n",
    "    \n",
    "    # Get all forget stages\n",
    "    forget_stages = []\n",
    "    for stage_dir in os.listdir(method_dir):\n",
    "        stage_path = os.path.join(method_dir, stage_dir)\n",
    "        if os.path.isdir(stage_path):\n",
    "            try:\n",
    "                # Handle both underscore and hyphen formats\n",
    "                if '-' in stage_dir:\n",
    "                    begin, end = map(int, stage_dir.split('-'))\n",
    "                elif '_' in stage_dir:\n",
    "                    begin, end = map(int, stage_dir.split('_'))\n",
    "                else:\n",
    "                    # Skip directories that don't follow either pattern\n",
    "                    continue\n",
    "                    \n",
    "                forget_stages.append((begin, end, stage_dir, stage_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping directory with invalid format: {stage_dir} - {str(e)}\")\n",
    "    \n",
    "    # Sort by end class for proper ordering\n",
    "    forget_stages.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Load class-wise accuracy data for each stage\n",
    "    for begin, end, stage_dir, stage_path in forget_stages:\n",
    "        eval_file = os.path.join(stage_path, 'evaluation_results.json')\n",
    "        if os.path.exists(eval_file):\n",
    "            try:\n",
    "                with open(eval_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # Check if class-wise accuracy data exists in the new format\n",
    "                    if 'class_wise_accuracy' in data:\n",
    "                        class_wise_items = data['class_wise_accuracy']\n",
    "                        \n",
    "                        # Extract test accuracy (filter by dataset=\"test\")\n",
    "                        test_items = [item for item in class_wise_items if item.get('dataset') == 'test']\n",
    "                        \n",
    "                        # Create dictionary mapping class index to accuracy\n",
    "                        class_acc = {}\n",
    "                        max_class = 0\n",
    "                        for item in test_items:\n",
    "                            class_idx = item.get('class')\n",
    "                            accuracy = item.get('accuracy')\n",
    "                            class_acc[class_idx] = accuracy\n",
    "                            max_class = max(max_class, class_idx)\n",
    "                        \n",
    "                        # Convert to list format for compatibility\n",
    "                        acc_list = [class_acc.get(i, 0) for i in range(max_class + 1)]\n",
    "                        \n",
    "                        classwise_data[end] = {\n",
    "                            'forget_class_begin': begin,\n",
    "                            'forget_class_end': end,\n",
    "                            'classes_forgotten': end - begin,\n",
    "                            'classwise_accuracy': acc_list\n",
    "                        }\n",
    "                    elif 'classwise_accuracy' in data:\n",
    "                        # Handle the original format if present\n",
    "                        classwise_acc = data['classwise_accuracy']\n",
    "                        classwise_data[end] = {\n",
    "                            'forget_class_begin': begin,\n",
    "                            'forget_class_end': end,\n",
    "                            'classes_forgotten': end - begin,\n",
    "                            'classwise_accuracy': classwise_acc\n",
    "                        }\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {eval_file}: {e}\")\n",
    "    \n",
    "    return classwise_data\n",
    "\n",
    "def plot_classwise_accuracy(method, dataset, method_display=None):\n",
    "    \"\"\"Plot class-wise accuracy for a specific method and dataset\"\"\"\n",
    "    if method_display is None:\n",
    "        method_display = method.replace('_continual_unlearn', '')\n",
    "        \n",
    "    classwise_data = extract_classwise_accuracy(method, dataset)\n",
    "    \n",
    "    if not classwise_data:\n",
    "        # Try a fallback approach: calculate class-wise accuracy from test dataloader\n",
    "        print(f\"No class-wise accuracy data found for {method} on {dataset}. Checking if we can extract from raw evaluation data...\")\n",
    "        \n",
    "        # Try to load data from test_predictions.npz if available\n",
    "        classwise_data = extract_classwise_accuracy_from_predictions(method, dataset)\n",
    "        \n",
    "        # Check if we can calculate class-wise accuracy now\n",
    "        if not classwise_data:\n",
    "            print(f\"Cannot generate class-wise accuracy plot for {method} on {dataset}\")\n",
    "            return\n",
    "    \n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Determine max number of classes based on dataset\n",
    "    max_classes = 10 if dataset == 'cifar10' else 100\n",
    "    \n",
    "    # Color map for different stages\n",
    "    cmap = plt.cm.get_cmap('viridis', len(classwise_data) + 1)\n",
    "    \n",
    "    # Plot class-wise accuracy for each stage\n",
    "    for i, (stage, stage_data) in enumerate(sorted(classwise_data.items())):\n",
    "        class_acc = stage_data['classwise_accuracy']\n",
    "        classes_forgotten = stage_data['classes_forgotten']\n",
    "        \n",
    "        # Convert to right format if it's a dict\n",
    "        if isinstance(class_acc, dict):\n",
    "            class_indices = sorted([int(k) for k in class_acc.keys()])\n",
    "            accuracy_values = [class_acc[str(k)] for k in class_indices]\n",
    "        else:  # Assume it's a list\n",
    "            class_indices = range(len(class_acc))\n",
    "            accuracy_values = class_acc\n",
    "        \n",
    "        # Plot with a line style that distinguishes forgotten vs. retained\n",
    "        plt.plot(class_indices, accuracy_values, 'o-', \n",
    "                label=f'Classes forgotten: {classes_forgotten}',\n",
    "                color=cmap(i), linewidth=2, markersize=6)\n",
    "        \n",
    "        # Mark forgotten classes with a different marker\n",
    "        forgotten_classes = range(stage_data['forget_class_begin'], stage_data['forget_class_end'])\n",
    "        forgotten_indices = [i for i in class_indices if i in forgotten_classes]\n",
    "        forgotten_values = [accuracy_values[class_indices.index(i)] for i in forgotten_indices if i in class_indices]\n",
    "        \n",
    "        if forgotten_indices:\n",
    "            plt.plot(forgotten_indices, forgotten_values, 'x', \n",
    "                    color=cmap(i), markersize=10, markeredgewidth=2)\n",
    "    \n",
    "    # Add horizontal lines showing forgetting thresholds (e.g., at 20% and 50%)\n",
    "    plt.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(y=20, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.title(f'Class-wise Accuracy for {method_display} on {dataset}')\n",
    "    plt.xlabel('Class Index')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(0, max_classes, 5 if max_classes > 20 else 1))\n",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Alternative implementation if classwise_accuracy isn't available directly\n",
    "def extract_classwise_accuracy_from_predictions(method, dataset):\n",
    "    \"\"\"Extract class-wise accuracy from raw prediction data\"\"\"\n",
    "    classwise_data = {}\n",
    "    method_dir = os.path.join(base_dir, method, dataset)\n",
    "    \n",
    "    if not os.path.exists(method_dir):\n",
    "        return classwise_data\n",
    "    \n",
    "    for stage_dir in os.listdir(method_dir):\n",
    "        stage_path = os.path.join(method_dir, stage_dir)\n",
    "        if os.path.isdir(stage_path):\n",
    "            try:\n",
    "                # Parse stage info\n",
    "                if '-' in stage_dir:\n",
    "                    begin, end = map(int, stage_dir.split('-'))\n",
    "                elif '_' in stage_dir:\n",
    "                    begin, end = map(int, stage_dir.split('_'))\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Check for predictions file\n",
    "                preds_file = os.path.join(stage_path, 'test_predictions.npz')\n",
    "                if os.path.exists(preds_file):\n",
    "                    # Load predictions and calculate class-wise accuracy\n",
    "                    data = np.load(preds_file)\n",
    "                    preds = data['preds']\n",
    "                    targets = data['targets']\n",
    "                    \n",
    "                    # Calculate per-class accuracy\n",
    "                    max_class = max(targets) + 1\n",
    "                    class_acc = []\n",
    "                    for c in range(max_class):\n",
    "                        class_indices = targets == c\n",
    "                        if np.sum(class_indices) > 0:  # Ensure we have samples for this class\n",
    "                            class_correct = np.sum(preds[class_indices] == c)\n",
    "                            class_total = np.sum(class_indices)\n",
    "                            accuracy = 100 * class_correct / class_total\n",
    "                        else:\n",
    "                            accuracy = 0\n",
    "                        class_acc.append(accuracy)\n",
    "                    \n",
    "                    classwise_data[end] = {\n",
    "                        'forget_class_begin': begin,\n",
    "                        'forget_class_end': end,\n",
    "                        'classes_forgotten': end - begin,\n",
    "                        'classwise_accuracy': class_acc\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {stage_path}: {e}\")\n",
    "    \n",
    "    return classwise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c405e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Class-wise Accuracy for CIFAR-10 ====\n",
      "\n",
      "Generating plot for FT...\n",
      "No class-wise accuracy data found for FT_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for FT_continual_unlearn on cifar10\n",
      "Generating plot for retrain...\n",
      "No class-wise accuracy data found for retrain_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for retrain_continual_unlearn on cifar10\n",
      "Generating plot for NG...\n",
      "No class-wise accuracy data found for NG_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for NG_continual_unlearn on cifar10\n",
      "Generating plot for synaptag...\n",
      "No class-wise accuracy data found for synaptag_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for synaptag_continual_unlearn on cifar10\n",
      "Generating plot for boundary_shrink...\n",
      "No class-wise accuracy data found for boundary_shrink_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for boundary_shrink_continual_unlearn on cifar10\n",
      "Generating plot for GA...\n",
      "No class-wise accuracy data found for GA_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for GA_continual_unlearn on cifar10\n",
      "Generating plot for RL...\n",
      "No class-wise accuracy data found for RL_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for RL_continual_unlearn on cifar10\n",
      "Generating plot for boundary_expanding...\n",
      "No class-wise accuracy data found for boundary_expanding_continual_unlearn on cifar10. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for boundary_expanding_continual_unlearn on cifar10\n",
      "\n",
      "==== Class-wise Accuracy for CIFAR-100 ====\n",
      "\n",
      "Generating plot for FT...\n",
      "No class-wise accuracy data found for FT_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for FT_continual_unlearn on cifar100\n",
      "Generating plot for retrain...\n",
      "No class-wise accuracy data found for retrain_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for retrain_continual_unlearn on cifar100\n",
      "Generating plot for NG...\n",
      "No class-wise accuracy data found for NG_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for NG_continual_unlearn on cifar100\n",
      "Generating plot for synaptag...\n",
      "No class-wise accuracy data found for synaptag_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for synaptag_continual_unlearn on cifar100\n",
      "Generating plot for boundary_shrink...\n",
      "No class-wise accuracy data found for boundary_shrink_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for boundary_shrink_continual_unlearn on cifar100\n",
      "Generating plot for GA...\n",
      "No class-wise accuracy data found for GA_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for GA_continual_unlearn on cifar100\n",
      "Generating plot for RL...\n",
      "No class-wise accuracy data found for RL_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for RL_continual_unlearn on cifar100\n",
      "Generating plot for boundary_expanding...\n",
      "No class-wise accuracy data found for boundary_expanding_continual_unlearn on cifar100. Checking if we can extract from raw evaluation data...\n",
      "Cannot generate class-wise accuracy plot for boundary_expanding_continual_unlearn on cifar100\n"
     ]
    }
   ],
   "source": [
    "# Plot class-wise accuracy for all methods on CIFAR-10\n",
    "if 'cifar10' in df['dataset'].unique():\n",
    "    print(\"\\n==== Class-wise Accuracy for CIFAR-10 ====\\n\")\n",
    "    \n",
    "    for method in methods:\n",
    "        method_display = method.replace('_continual_unlearn', '')\n",
    "        print(f\"Generating plot for {method_display}...\")\n",
    "        plot_classwise_accuracy(method, 'cifar10', method_display)\n",
    "\n",
    "# Plot class-wise accuracy for all methods on CIFAR-100\n",
    "if 'cifar100' in df['dataset'].unique():\n",
    "    print(\"\\n==== Class-wise Accuracy for CIFAR-100 ====\\n\")\n",
    "    \n",
    "    for method in methods:\n",
    "        method_display = method.replace('_continual_unlearn', '')\n",
    "        print(f\"Generating plot for {method_display}...\")\n",
    "        plot_classwise_accuracy(method, 'cifar100', method_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03568d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classwise_comparison(dataset, target_stage=None):\n",
    "    \"\"\"Compare class-wise accuracy across methods at a specific forget stage\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Determine max number of classes based on dataset\n",
    "    max_classes = 10 if dataset == 'cifar10' else 100\n",
    "    \n",
    "    # Collect class-wise data for all methods\n",
    "    method_data = {}\n",
    "    for method in methods:\n",
    "        method_display = method.replace('_continual_unlearn', '')\n",
    "        classwise_data = extract_classwise_accuracy(method, dataset)\n",
    "        \n",
    "        if not classwise_data:\n",
    "            classwise_data = extract_classwise_accuracy_from_predictions(method, dataset)\n",
    "            \n",
    "        if classwise_data:\n",
    "            # Select the right stage to compare\n",
    "            if target_stage is None:\n",
    "                # Default to the maximum common stage\n",
    "                available_stages = list(classwise_data.keys())\n",
    "                if available_stages:\n",
    "                    selected_stage = max(available_stages)\n",
    "                    method_data[method_display] = classwise_data[selected_stage]\n",
    "            elif target_stage in classwise_data:\n",
    "                method_data[method_display] = classwise_data[target_stage]\n",
    "            else:\n",
    "                # If exact target stage not found, find the closest one\n",
    "                available_stages = sorted(list(classwise_data.keys()))\n",
    "                if available_stages:\n",
    "                    closest_stage = min(available_stages, key=lambda x: abs(x - target_stage))\n",
    "                    method_data[method_display] = classwise_data[closest_stage]\n",
    "                    print(f\"Note: For method {method_display}, using stage {closest_stage} instead of requested {target_stage}\")\n",
    "    \n",
    "    if not method_data:\n",
    "        print(f\"No class-wise data available for comparison on {dataset}\")\n",
    "        return\n",
    "    \n",
    "    # Determine the stage being compared\n",
    "    example_data = next(iter(method_data.values()))\n",
    "    classes_forgotten = example_data['classes_forgotten']\n",
    "    stage_end = example_data['forget_class_end']\n",
    "    \n",
    "    # Plot class-wise accuracy for each method\n",
    "    for method_display, data in method_data.items():\n",
    "        class_acc = data['classwise_accuracy']\n",
    "        \n",
    "        # Convert to right format if it's a dict\n",
    "        if isinstance(class_acc, dict):\n",
    "            class_indices = sorted([int(k) for k in class_acc.keys()])\n",
    "            accuracy_values = [class_acc[str(k)] for k in class_indices]\n",
    "        else:  # Assume it's a list\n",
    "            class_indices = range(len(class_acc))\n",
    "            accuracy_values = class_acc\n",
    "        \n",
    "        # Plot with a unique style for each method\n",
    "        plt.plot(class_indices, accuracy_values, 'o-', \n",
    "                label=method_display,\n",
    "                linewidth=2, markersize=6)\n",
    "        \n",
    "    # Add vertical band to highlight forgotten classes\n",
    "    plt.axvspan(0, stage_end, color='lightgray', alpha=0.3, label=f'Forgotten ({stage_end} classes)')\n",
    "    \n",
    "    # Add horizontal lines showing thresholds\n",
    "    plt.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(y=20, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.title(f'Class-wise Accuracy Comparison on {dataset} (Forgotten classes: {classes_forgotten})')\n",
    "    plt.xlabel('Class Index')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(0, max_classes, 5 if max_classes > 20 else 1))\n",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run class-wise accuracy extraction on GPU cluster with many files\n",
    "def run_batch_classwise_extraction():\n",
    "    \"\"\"Extract and export class-wise accuracy data for all methods to a single file\"\"\"\n",
    "    print(\"Running batch extraction of class-wise accuracy data...\")\n",
    "    \n",
    "    all_classwise_data = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        method_display = method.replace('_continual_unlearn', '')\n",
    "        all_classwise_data[method_display] = {}\n",
    "        \n",
    "        for dataset in datasets[method]:\n",
    "            print(f\"Extracting data for {method_display} on {dataset}...\")\n",
    "            classwise_data = extract_classwise_accuracy(method, dataset)\n",
    "            \n",
    "            if not classwise_data:\n",
    "                print(f\"  Falling back to prediction-based extraction...\")\n",
    "                classwise_data = extract_classwise_accuracy_from_predictions(method, dataset)\n",
    "            \n",
    "            if classwise_data:\n",
    "                all_classwise_data[method_display][dataset] = classwise_data\n",
    "                print(f\"  Successfully extracted data for {len(classwise_data)} stages\")\n",
    "            else:\n",
    "                print(f\"  No class-wise data found\")\n",
    "    \n",
    "    # Save the extracted data to a file\n",
    "    output_dir = os.path.join(os.path.dirname(base_dir), \"plots\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, \"classwise_accuracy_data.npz\")\n",
    "    \n",
    "    # Convert complex nested dict to a simpler format for saving\n",
    "    save_dict = {}\n",
    "    for method, datasets_dict in all_classwise_data.items():\n",
    "        for dataset, stages_dict in datasets_dict.items():\n",
    "            for stage, data in stages_dict.items():\n",
    "                key = f\"{method}_{dataset}_{stage}\"\n",
    "                save_dict[f\"{key}_begin\"] = data['forget_class_begin']\n",
    "                save_dict[f\"{key}_end\"] = data['forget_class_end']\n",
    "                save_dict[f\"{key}_forgotten\"] = data['classes_forgotten']\n",
    "                save_dict[f\"{key}_accuracy\"] = np.array(data['classwise_accuracy'])\n",
    "    \n",
    "    np.savez_compressed(output_file, **save_dict)\n",
    "    print(f\"Saved class-wise accuracy data to {output_file}\")\n",
    "    \n",
    "    return all_classwise_data\n",
    "\n",
    "# Run the batch extraction\n",
    "# Uncomment to run on the computing cluster\n",
    "# all_classwise_data = run_batch_classwise_extraction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
