{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "# Continual Unlearning Results Visualization\n",
                "\n",
                "This notebook visualizes the results of continual unlearning experiments across different models (FT, GA, RL) as they progressively forget more classes."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Import required libraries\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import glob\n",
                "from pathlib import Path\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('ggplot')\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = [12, 8]\n",
                "plt.rcParams['font.size'] = 12"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Data Loading and Processing\n",
                "\n",
                "First, we'll load all the CSV files from the checkpoints directory and organize them by model type and forgetting stages."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Define the base path for checkpoints\n",
                "base_path = \"/home/kdkyum/workdir/cl_unlearn/checkpoints/\"\n",
                "\n",
                "# Dictionary to store results by model type\n",
                "all_results = {\n",
                "    'base': None,\n",
                "    'FT': {},\n",
                "    'GA': {},\n",
                "    'RL': {}\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Load base model results\n",
                "base_model_path = os.path.join(base_path, \"base_model/class_wise_accuracy.csv\")\n",
                "if os.path.exists(base_model_path):\n",
                "    all_results['base'] = pd.read_csv(base_model_path)\n",
                "    print(f\"Loaded base model data: {all_results['base'].shape[0]} rows\")\n",
                "else:\n",
                "    print(\"Base model data not found\")"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Function to extract number of forgotten classes from directory path\n",
                "def extract_forgotten_classes(path):\n",
                "    # Extract directory name\n",
                "    dir_name = os.path.basename(os.path.dirname(path))\n",
                "    \n",
                "    # Count the number of 'c' characters in the directory name\n",
                "    if dir_name.startswith(\"forget_class_\"):\n",
                "        forgotten_classes = dir_name.replace(\"forget_class_\", \"\").split(\"_\")\n",
                "        return len(forgotten_classes)\n",
                "    return 0\n",
                "\n",
                "# Function to get forgotten class numbers\n",
                "def get_forgotten_classes(path):\n",
                "    # Extract directory name\n",
                "    dir_name = os.path.basename(os.path.dirname(path))\n",
                "    \n",
                "    # Extract class numbers\n",
                "    if dir_name.startswith(\"forget_class_\"):\n",
                "        class_str = dir_name.replace(\"forget_class_\", \"\")\n",
                "        return [int(c.replace('c', '')) for c in class_str.split(\"_\")]\n",
                "    return []\n",
                "\n",
                "# Load all model results\n",
                "for model_type in ['FT', 'GA', 'RL']:\n",
                "    model_path = os.path.join(base_path, f\"{model_type}_model/\")\n",
                "    if os.path.exists(model_path):\n",
                "        # Find all CSV files in subdirectories\n",
                "        csv_files = glob.glob(os.path.join(model_path, \"**/class_wise_accuracy.csv\"), recursive=True)\n",
                "        \n",
                "        for file_path in csv_files:\n",
                "            # Extract number of forgotten classes\n",
                "            num_forgotten = extract_forgotten_classes(file_path)\n",
                "            forgotten_classes = get_forgotten_classes(file_path)\n",
                "            \n",
                "            # Load data\n",
                "            df = pd.read_csv(file_path)\n",
                "            df['num_forgotten_classes'] = num_forgotten\n",
                "            df['forgotten_classes'] = str(forgotten_classes)\n",
                "            df['model_type'] = model_type\n",
                "            \n",
                "            # Store in dictionary\n",
                "            all_results[model_type][num_forgotten] = df\n",
                "            \n",
                "        print(f\"Loaded {len(all_results[model_type])} datasets for {model_type} model\")"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Combine all datasets for analysis\n",
                "combined_data = []\n",
                "\n",
                "# Add base model data\n",
                "if all_results['base'] is not None:\n",
                "    base_df = all_results['base'].copy()\n",
                "    base_df['num_forgotten_classes'] = 0\n",
                "    base_df['forgotten_classes'] = '[]'\n",
                "    base_df['model_type'] = 'base'\n",
                "    combined_data.append(base_df)\n",
                "\n",
                "# Add all other model data\n",
                "for model_type in ['FT', 'GA', 'RL']:\n",
                "    for num_forgotten, df in all_results[model_type].items():\n",
                "        combined_data.append(df)\n",
                "\n",
                "# Combine into single dataframe\n",
                "if combined_data:\n",
                "    all_data = pd.concat(combined_data, ignore_index=True)\n",
                "    print(f\"Combined dataframe shape: {all_data.shape}\")\n",
                "    \n",
                "    # Add flag for forgotten vs retained classes\n",
                "    all_data['is_forgotten'] = all_data.apply(\n",
                "        lambda row: row['class'] in eval(row['forgotten_classes']) \n",
                "        if row['forgotten_classes'] != '[]' else False, \n",
                "        axis=1\n",
                "    )\n",
                "else:\n",
                "    print(\"No data to combine\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Visualization 1: Accuracy of Forgotten vs Retained Classes\n",
                "\n",
                "Let's visualize how well the models forget the targeted classes while retaining performance on other classes."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Filter to only test set results\n",
                "test_data = all_data[all_data['dataset'] == 'test'].copy()\n",
                "\n",
                "# Create a summary dataframe\n",
                "summary = test_data.groupby(['model_type', 'num_forgotten_classes', 'is_forgotten'])['accuracy'].mean().reset_index()\n",
                "\n",
                "# Plot retained vs forgotten class accuracy\n",
                "plt.figure(figsize=(14, 8))\n",
                "\n",
                "for model_type in sorted(summary['model_type'].unique()):\n",
                "    if model_type == 'base':\n",
                "        continue\n",
                "    \n",
                "    # Forgotten classes\n",
                "    forgotten = summary[(summary['model_type'] == model_type) & (summary['is_forgotten'])]\n",
                "    plt.plot(forgotten['num_forgotten_classes'], forgotten['accuracy'], \n",
                "             marker='o', linestyle='-', label=f'{model_type} - Forgotten Classes')\n",
                "    \n",
                "    # Retained classes\n",
                "    retained = summary[(summary['model_type'] == model_type) & (~summary['is_forgotten'])]\n",
                "    plt.plot(retained['num_forgotten_classes'], retained['accuracy'], \n",
                "             marker='s', linestyle='--', label=f'{model_type} - Retained Classes')\n",
                "\n",
                "plt.title('Test Accuracy: Forgotten vs. Retained Classes')\n",
                "plt.xlabel('Number of Forgotten Classes')\n",
                "plt.ylabel('Average Accuracy (%)')\n",
                "plt.grid(True)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Visualization 2: Class-wise Accuracy Heatmaps\n",
                "\n",
                "Let's create heatmaps to visualize the accuracy of each class as more classes are forgotten."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Function to create heatmap for a specific model type\n",
                "def plot_heatmap(model_type, dataset_type='test'):\n",
                "    plt.figure(figsize=(14, 8))\n",
                "    \n",
                "    # Filter data\n",
                "    filtered_data = all_data[(all_data['model_type'] == model_type) & \n",
                "                            (all_data['dataset'] == dataset_type)]\n",
                "    \n",
                "    if filtered_data.empty:\n",
                "        print(f\"No data available for {model_type} model\")\n",
                "        return\n",
                "    \n",
                "    # Create pivot table for heatmap\n",
                "    heatmap_data = filtered_data.pivot_table(\n",
                "        index='num_forgotten_classes', \n",
                "        columns='class', \n",
                "        values='accuracy'\n",
                "    )\n",
                "    \n",
                "    # Sort by number of forgotten classes\n",
                "    heatmap_data = heatmap_data.sort_index()\n",
                "    \n",
                "    # Create heatmap\n",
                "    sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', vmin=0, vmax=100)\n",
                "    \n",
                "    plt.title(f'{model_type} Model - Class-wise Accuracy ({dataset_type} set)')\n",
                "    plt.xlabel('Class')\n",
                "    plt.ylabel('Number of Forgotten Classes')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "# Plot heatmaps for each model type\n",
                "for model_type in ['FT', 'GA', 'RL']:\n",
                "    if all_results[model_type]:\n",
                "        plot_heatmap(model_type)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Visualization 3: Comparing Different Methods\n",
                "\n",
                "Let's compare the performance of different unlearning methods (FT, GA, RL)."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Calculate the average accuracy for forgotten and retained classes\n",
                "model_comparison = test_data.groupby(['model_type', 'num_forgotten_classes', 'is_forgotten'])['accuracy'].mean().reset_index()\n",
                "\n",
                "# Plot comparison for forgotten classes\n",
                "plt.figure(figsize=(14, 6))\n",
                "\n",
                "# Forgotten classes\n",
                "forgotten_comparison = model_comparison[model_comparison['is_forgotten']]\n",
                "for model_type in sorted(forgotten_comparison['model_type'].unique()):\n",
                "    if model_type == 'base':\n",
                "        continue\n",
                "    model_data = forgotten_comparison[forgotten_comparison['model_type'] == model_type]\n",
                "    plt.plot(model_data['num_forgotten_classes'], model_data['accuracy'], \n",
                "             marker='o', linestyle='-', linewidth=2, label=model_type)\n",
                "\n",
                "plt.title('Comparing Methods: Average Accuracy on Forgotten Classes')\n",
                "plt.xlabel('Number of Forgotten Classes')\n",
                "plt.ylabel('Average Accuracy (%)')\n",
                "plt.grid(True)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Plot comparison for retained classes\n",
                "plt.figure(figsize=(14, 6))\n",
                "\n",
                "# Retained classes\n",
                "retained_comparison = model_comparison[~model_comparison['is_forgotten']]\n",
                "for model_type in sorted(retained_comparison['model_type'].unique()):\n",
                "    if model_type == 'base':\n",
                "        continue\n",
                "    model_data = retained_comparison[retained_comparison['model_type'] == model_type]\n",
                "    plt.plot(model_data['num_forgotten_classes'], model_data['accuracy'], \n",
                "             marker='s', linestyle='--', linewidth=2, label=model_type)\n",
                "\n",
                "# Add base model for reference (if available)\n",
                "if 'base' in model_comparison['model_type'].unique():\n",
                "    base_acc = model_comparison[model_comparison['model_type'] == 'base']['accuracy'].mean()\n",
                "    plt.axhline(y=base_acc, color='black', linestyle='-', label='Base Model')\n",
                "\n",
                "plt.title('Comparing Methods: Average Accuracy on Retained Classes')\n",
                "plt.xlabel('Number of Forgotten Classes')\n",
                "plt.ylabel('Average Accuracy (%)')\n",
                "plt.grid(True)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Visualization 4: Forgetting Efficiency\n",
                "\n",
                "Let's create a combined metric to evaluate how efficiently each method forgets classes while maintaining accuracy on retained classes."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Calculate forgetting efficiency\n",
                "# A good forgetting method should have low accuracy on forgotten classes and high accuracy on retained classes\n",
                "\n",
                "forgetting_efficiency = []\n",
                "\n",
                "for model_type in ['FT', 'GA', 'RL']:\n",
                "    for num_forgotten in sorted(all_results[model_type].keys()):\n",
                "        # Skip if no data\n",
                "        if num_forgotten not in all_results[model_type]:\n",
                "            continue\n",
                "            \n",
                "        # Filter for test data\n",
                "        df = all_results[model_type][num_forgotten]\n",
                "        df_test = df[df['dataset'] == 'test']\n",
                "        \n",
                "        # Get forgotten classes\n",
                "        forgotten_classes = eval(df_test['forgotten_classes'].iloc[0]) if not df_test.empty else []\n",
                "        \n",
                "        # Calculate metrics\n",
                "        forgotten_acc = df_test[df_test['class'].isin(forgotten_classes)]['accuracy'].mean() if forgotten_classes else np.nan\n",
                "        retained_acc = df_test[~df_test['class'].isin(forgotten_classes)]['accuracy'].mean() if forgotten_classes else np.nan\n",
                "        \n",
                "        # Efficiency score (higher is better)\n",
                "        # We want low accuracy on forgotten classes and high accuracy on retained classes\n",
                "        efficiency = retained_acc - forgotten_acc if not np.isnan(forgotten_acc) and not np.isnan(retained_acc) else np.nan\n",
                "        \n",
                "        forgetting_efficiency.append({\n",
                "            'model_type': model_type,\n",
                "            'num_forgotten_classes': num_forgotten,\n",
                "            'forgotten_accuracy': forgotten_acc,\n",
                "            'retained_accuracy': retained_acc,\n",
                "            'efficiency_score': efficiency\n",
                "        })\n",
                "\n",
                "# Convert to dataframe\n",
                "efficiency_df = pd.DataFrame(forgetting_efficiency)\n",
                "\n",
                "# Plot efficiency scores\n",
                "plt.figure(figsize=(14, 6))\n",
                "\n",
                "for model_type in sorted(efficiency_df['model_type'].unique()):\n",
                "    model_data = efficiency_df[efficiency_df['model_type'] == model_type]\n",
                "    plt.plot(model_data['num_forgotten_classes'], model_data['efficiency_score'], \n",
                "             marker='D', linestyle='-', linewidth=2, label=model_type)\n",
                "\n",
                "plt.title('Forgetting Efficiency Score by Method (Higher is Better)')\n",
                "plt.xlabel('Number of Forgotten Classes')\n",
                "plt.ylabel('Efficiency Score (Retained Acc - Forgotten Acc)')\n",
                "plt.grid(True)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Visualization 5: Progressive Forgetting Impact\n",
                "\n",
                "Let's see how the accuracy of specific classes changes as more and more classes are forgotten."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python"
            },
            "source": [
                "# Select classes for visualization\n",
                "selected_classes = [0, 5, 9]  # Example classes (first forgotten, middle, last one)\n",
                "\n",
                "# For each model type, track the accuracy of selected classes\n",
                "plt.figure(figsize=(15, 10))\n",
                "\n",
                "for model_type in ['FT', 'GA']:\n",
                "    for class_num in selected_classes:\n",
                "        class_accuracies = []\n",
                "        forget_steps = []\n",
                "        \n",
                "        for num_forgotten, df in sorted(all_results[model_type].items()):\n",
                "            if df.empty:\n",
                "                continue\n",
                "                \n",
                "            # Get test accuracy for the specific class\n",
                "            class_data = df[(df['dataset'] == 'test') & (df['class'] == class_num)]\n",
                "            if not class_data.empty:\n",
                "                class_accuracies.append(class_data['accuracy'].iloc[0])\n",
                "                forget_steps.append(num_forgotten)\n",
                "        \n",
                "        plt.plot(forget_steps, class_accuracies, \n",
                "                 marker='o' if class_num == 0 else ('s' if class_num == 5 else 'D'), \n",
                "                 linestyle='-', linewidth=2, \n",
                "                 label=f'{model_type} - Class {class_num}')\n",
                "\n",
                "plt.title('Progressive Forgetting: Accuracy of Selected Classes')\n",
                "plt.xlabel('Number of Forgotten Classes')\n",
                "plt.ylabel('Accuracy (%)')\n",
                "plt.grid(True)\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "language": "markdown"
            },
            "source": [
                "## Summary and Conclusions\n",
                "\n",
                "Based on the visualizations, we can draw the following conclusions about the continual unlearning performance:\n",
                "\n",
                "1. **Effectiveness at Forgetting:** The visualizations show how effectively each method (FT, GA, RL) can make the model forget specific classes. Lower accuracy on forgotten classes indicates better forgetting.\n",
                "\n",
                "2. **Preservation of Retained Knowledge:** We can see how well each method preserves the model's performance on retained classes while forgetting others.\n",
                "\n",
                "3. **Scalability with More Forgetting:** The trends show how each method performs as more and more classes need to be forgotten.\n",
                "\n",
                "4. **Method Comparison:** The efficiency score helps identify which method provides the best balance between forgetting targeted classes and retaining performance on others.\n",
                "\n",
                "5. **Impact on Specific Classes:** The class-wise visualizations reveal how the accuracy of specific classes changes throughout the continual unlearning process."
            ]
        }
    ]
}